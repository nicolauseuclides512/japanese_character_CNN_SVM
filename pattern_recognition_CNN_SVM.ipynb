{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, Adadelta, Adagrad, Adamax, Ftrl, Nadam, RMSprop\n",
    "from tensorflow.keras.losses import categorical_crossentropy, MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, concatenate, Input, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_dir = 'resize63/train'\n",
    "# test_dir = 'resize63/test'\n",
    "train_dir = 'resize6/train'\n",
    "test_dir = 'resize6/test'\n",
    "# train_dir = 'resize69/train'\n",
    "# test_dir = 'resize69/test'\n",
    "hiragana_train_dir = train_dir+\"/Hiragana\"\n",
    "katakana_train_dir = train_dir+\"/Katakana\"\n",
    "hiragana_test_dir = test_dir+\"/Hiragana\"\n",
    "katakana_test_dir = test_dir+\"/Katakana\"\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "# data_dir = '../thesis_program/data_image_new'\n",
    "# hiragana_dir = data_dir+\"/Hiragana\"\n",
    "# katakana_dir = data_dir+\"/Katakana\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "letter_labels = [\n",
    "    [0, \"A\", \"A\"], [1, \"Ba\", \"Ba\"], [2, \"Be\", \"Be\"], [3, \"Bi\", \"Bi\"], [4, \"Bo\", \"Bo\"], [5, \"Bu\", \"Bu\"],\n",
    "    [6, \"Bya\", \"Bya\"], [7, \"Byo\", \"Byo\"], [8, \"Byu\", \"Byu\"], [9, \"Cha\", \"Cha\"], [10, \"Chi\", \"Chi\"],\n",
    "    [11, \"Cho\", \"Cho\"], [12, \"Chu\", \"Chu\"], [13, \"Da\", \"Da\"], [14, \"De\", \"De\"], [15, \"Do\", \"Do\"],\n",
    "    [16, \"E\", \"E\"], [17, \"Fu\", \"Fu\"], [18, \"Ga\", \"Ga\"], [19, \"Ge\", \"Ge\"], [20, \"Gi\", \"Gi\"],\n",
    "    [21, \"Go\", \"Go\"], [22, \"Gu\", \"Gu\"], [23, \"Gya\", \"Gya\"], [24, \"Gyo\", \"Gyo\"], [25, \"Gyu\", \"Gyu\"],\n",
    "    [26, \"Ha\", \"Ha\"], [27, \"He\", \"He\"], [28, \"Hi\", \"Hi\"], [29, \"Ho\", \"Ho\"], [30, \"Hya\", \"Hya\"],\n",
    "    [31, \"Hyo\", \"Hyo\"], [32, \"Hyu\", \"Hyu\"], [33, \"I\", \"I\"], [34, \"Ja\", \"Ja\"], [35, \"Ji\", \"Ji\"],\n",
    "    [36, \"Ji_2\", \"Ji\"], [37, \"Jo\", \"Jo\"], [38, \"Ju\", \"Ju\"], [39, \"Ka\", \"Ka\"], [40, \"Ke\", \"Ke\"],\n",
    "    [41, \"Ki\", \"Ki\"], [42, \"Ko\", \"Ko\"], [43, \"Ku\", \"Ku\"], [44, \"Kya\", \"Kya\"], [45, \"Kyo\", \"Kyo\"],\n",
    "    [46, \"Kyu\", \"Kyu\"], [47, \"Ma\", \"Ma\"], [48, \"Me\", \"Me\"], [49, \"Mi\", \"Mi\"], [50, \"Mo\", \"Mo\"],\n",
    "    [51, \"Mu\", \"Mu\"], [52, \"Mya\", \"Mya\"], [53, \"Myo\", \"Myo\"], [54, \"Myu\", \"Myu\"], [55, \"N\", \"N\"],\n",
    "    [56, \"Na\", \"Na\"], [57, \"Ne\", \"Ne\"], [58, \"Ni\", \"Ni\"], [59, \"No\", \"No\"], [60, \"Nu\", \"Nu\"],\n",
    "    [61, \"Nya\", \"Nya\"], [62, \"Nyo\", \"Nyo\"], [63, \"Nyu\", \"Nyu\"], [64, \"O\", \"O\"], [65, \"Pa\", \"Pa\"],\n",
    "    [66, \"Pe\", \"Pe\"], [67, \"Pi\", \"Pi\"], [68, \"Po\", \"Po\"], [69, \"Pu\", \"Pu\"], [70, \"Pya\", \"Pya\"],\n",
    "    [71, \"Pyo\", \"Pyo\"], [72, \"Pyu\", \"Pyu\"], [73, \"Ra\", \"Ra\"], [74, \"Re\", \"Re\"], [75, \"Ri\", \"Ri\"],\n",
    "    [76, \"Ro\", \"Ro\"], [77, \"Ru\", \"Ru\"], [78, \"Rya\", \"Rya\"], [79, \"Ryo\", \"Ryo\"], [80, \"Ryu\", \"Ryu\"],\n",
    "    [81, \"Sa\", \"Sa\"], [82, \"Se\", \"Se\"], [83, \"Sha\", \"Sha\"], [84, \"Shi\", \"Shi\"], [85, \"Sho\", \"Sho\"],\n",
    "    [86, \"Shu\", \"Shu\"], [87, \"So\", \"So\"], [88, \"Su\", \"Su\"], [89, \"Ta\", \"Ta\"], [90, \"Te\", \"Te\"],\n",
    "    [91, \"To\", \"To\"], [92, \"Tsu\", \"Tsu\"], [93, \"U\", \"U\"], [94, \"Wa\", \"Wa\"], [95, \"Wo\", \"Wo\"],\n",
    "    [96, \"Ya\", \"Ya\"], [97, \"Yo\", \"Yo\"], [98, \"Yu\", \"Yu\"], [99, \"Za\", \"Za\"], [100, \"Ze\", \"Ze\"],\n",
    "    [101, \"Zo\", \"Zo\"], [102, \"Zu\", \"Zu\"], [103, \"Zu_2\", \"Zu\"], [104, \"A\", \"A\"], [105, \"Ba\", \"Ba\"],\n",
    "    [106, \"Be\", \"Be\"], [107, \"Bi\", \"Bi\"], [108, \"Bo\", \"Bo\"], [109, \"Bu\", \"Bu\"], [110, \"Bya\", \"Bya\"],\n",
    "    [111, \"Byo\", \"Byo\"], [112, \"Byu\", \"Byu\"], [113, \"Cha\", \"Cha\"], [114, \"Che\", \"Che\"], [115, \"Chi\", \"Chi\"],\n",
    "    [116, \"Cho\", \"Cho\"], [117, \"Chu\", \"Chu\"], [118, \"Da\", \"Da\"], [119, \"De\", \"De\"], [120, \"Di\", \"Di\"],\n",
    "    [121, \"Do\", \"Do\"], [122, \"Du\", \"Du\"], [123, \"Dyu\", \"Dyu\"], [124, \"E\", \"E\"], [125, \"Fa\", \"Fa\"],\n",
    "    [126, \"Fe\", \"Fe\"], [127, \"Fi\", \"Fi\"], [128, \"Fo\", \"Fo\"], [129, \"Fu\", \"Fu\"], [130, \"Ga\", \"Ga\"],\n",
    "    [131, \"Ge\", \"Ge\"], [132, \"Gi\", \"Gi\"], [133, \"Go\", \"Go\"], [134, \"Gu\", \"Gu\"], [135, \"Gya\", \"Gya\"],\n",
    "    [136, \"Gyo\", \"Gyo\"], [137, \"Gyu\", \"Gyu\"], [138, \"Ha\", \"Ha\"], [139, \"He\", \"He\"], [140, \"Hi\", \"Hi\"],\n",
    "    [141, \"Ho\", \"Ho\"], [142, \"Hya\", \"Hya\"], [143, \"Hyo\", \"Hyo\"], [144, \"Hyu\", \"Hyu\"], [145, \"I\", \"I\"],\n",
    "    [146, \"Ja\", \"Ja\"], [147, \"Je\", \"Je\"], [148, \"Ji\", \"Ji\"], [149, \"Ji_2\", \"Ji\"], [150, \"Jo\", \"Jo\"],\n",
    "    [151, \"Ju\", \"Ju\"], [152, \"Ka\", \"Ka\"], [153, \"Ke\", \"Ke\"], [154, \"Ki\", \"Ki\"], [155, \"Ko\", \"Ko\"],\n",
    "    [156, \"Ku\", \"Ku\"], [157, \"Kya\", \"Kya\"], [158, \"Kyo\", \"Kyo\"], [159, \"Kyu\", \"Kyu\"], [160, \"Ma\", \"Ma\"],\n",
    "    [161, \"Me\", \"Me\"], [162, \"Mi\", \"Mi\"], [163, \"Mo\", \"Mo\"], [164, \"Mu\", \"Mu\"], [165, \"Mya\", \"Mya\"],\n",
    "    [166, \"Myo\", \"Myo\"], [167, \"Myu\", \"Myu\"], [168, \"N\", \"N\"], [169, \"Na\", \"Na\"], [170, \"Ne\", \"Ne\"],\n",
    "    [171, \"Ni\", \"Ni\"], [172, \"No\", \"No\"], [173, \"Nu\", \"Nu\"], [174, \"Nya\", \"Nya\"], [175, \"Nyo\", \"Nyo\"],\n",
    "    [176, \"Nyu\", \"Nyu\"], [177, \"O\", \"O\"], [178, \"Pa\", \"Pa\"], [179, \"Pe\", \"Pe\"], [180, \"Pi\", \"Pi\"],\n",
    "    [181, \"Po\", \"Po\"], [182, \"Pu\", \"Pu\"], [183, \"Pya\", \"Pya\"], [184, \"Pyo\", \"Pyo\"], [185, \"Pyu\", \"Pyu\"],\n",
    "    [186, \"Ra\", \"Ra\"], [187, \"Re\", \"Re\"], [188, \"Ri\", \"Ri\"], [189, \"Ro\", \"Ro\"], [190, \"Ru\", \"Ru\"],\n",
    "    [191, \"Rya\", \"Rya\"], [192, \"Ryo\", \"Ryo\"], [193, \"Ryu\", \"Ryu\"], [194, \"Sa\", \"Sa\"], [195, \"Se\", \"Se\"],\n",
    "    [196, \"Sha\", \"Sha\"], [197, \"She\", \"She\"], [198, \"Shi\", \"Shi\"], [199, \"Sho\", \"Sho\"], [200, \"Shu\", \"Shu\"],\n",
    "    [201, \"So\", \"So\"], [202, \"Su\", \"Su\"], [203, \"Ta\", \"Ta\"], [204, \"Te\", \"Te\"], [205, \"Ti\", \"Ti\"],\n",
    "    [206, \"To\", \"To\"], [207, \"Tsa\", \"Tsa\"], [208, \"Tse\", \"Tse\"], [209, \"Tso\", \"Tso\"], [210, \"Tsu\", \"Tsu\"],\n",
    "    [211, \"Tu\", \"Tu\"], [212, \"U\", \"U\"], [213, \"Wa\", \"Wa\"], [214, \"We\", \"We\"], [215, \"Wi\", \"Wi\"],\n",
    "    [216, \"Wo\", \"Wo\"], [217, \"Wo_2\", \"Wo\"], [218, \"Ya\", \"Ya\"], [219, \"Yo\", \"Yo\"], [220, \"Yu\", \"Yu\"],\n",
    "    [221, \"Za\", \"Za\"], [222, \"Ze\", \"Ze\"], [223, \"Zo\", \"Zo\"], [224, \"Zu\", \"Zu\"], [225, \"Zu_2\", \"Zu\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "A 46 images\n",
      "Ba 46 images\n",
      "Be 46 images\n",
      "Bi 46 images\n",
      "Bo 46 images\n",
      "Bu 46 images\n",
      "Bya 46 images\n",
      "Byo 46 images\n",
      "Byu 46 images\n",
      "Cha 46 images\n",
      "Chi 46 images\n",
      "Cho 46 images\n",
      "Chu 46 images\n",
      "Da 46 images\n",
      "De 46 images\n",
      "Do 46 images\n",
      "E 46 images\n",
      "Fu 46 images\n",
      "Ga 46 images\n",
      "Ge 46 images\n",
      "Gi 46 images\n",
      "Go 46 images\n",
      "Gu 46 images\n",
      "Gya 46 images\n",
      "Gyo 46 images\n",
      "Gyu 46 images\n",
      "Ha 46 images\n",
      "He 46 images\n",
      "Hi 46 images\n",
      "Ho 46 images\n",
      "Hya 46 images\n",
      "Hyo 46 images\n",
      "Hyu 46 images\n",
      "I 46 images\n",
      "Ja 46 images\n",
      "Ji 46 images\n",
      "Ji_2 46 images\n",
      "Jo 46 images\n",
      "Ju 46 images\n",
      "Ka 46 images\n",
      "Ke 46 images\n",
      "Ki 46 images\n",
      "Ko 46 images\n",
      "Ku 46 images\n",
      "Kya 46 images\n",
      "Kyo 46 images\n",
      "Kyu 46 images\n",
      "Ma 46 images\n",
      "Me 46 images\n",
      "Mi 46 images\n",
      "Mo 46 images\n",
      "Mu 46 images\n",
      "Mya 46 images\n",
      "Myo 46 images\n",
      "Myu 46 images\n",
      "N 46 images\n",
      "Na 46 images\n",
      "Ne 46 images\n",
      "Ni 46 images\n",
      "No 46 images\n",
      "Nu 46 images\n",
      "Nya 46 images\n",
      "Nyo 46 images\n",
      "Nyu 46 images\n",
      "O 46 images\n",
      "Pa 46 images\n",
      "Pe 46 images\n",
      "Pi 46 images\n",
      "Po 46 images\n",
      "Pu 46 images\n",
      "Pya 46 images\n",
      "Pyo 46 images\n",
      "Pyu 46 images\n",
      "Ra 46 images\n",
      "Re 46 images\n",
      "Ri 46 images\n",
      "Ro 46 images\n",
      "Ru 46 images\n",
      "Rya 46 images\n",
      "Ryo 46 images\n",
      "Ryu 46 images\n",
      "Sa 46 images\n",
      "Se 45 images\n",
      "Sha 46 images\n",
      "Shi 46 images\n",
      "Sho 46 images\n",
      "Shu 46 images\n",
      "So 46 images\n",
      "Su 46 images\n",
      "Ta 46 images\n",
      "Te 46 images\n",
      "To 46 images\n",
      "Tsu 46 images\n",
      "U 46 images\n",
      "Wa 46 images\n",
      "Wo 46 images\n",
      "Ya 46 images\n",
      "Yo 46 images\n",
      "Yu 46 images\n",
      "Za 46 images\n",
      "Ze 46 images\n",
      "Zo 46 images\n",
      "Zu 46 images\n",
      "Zu_2 46 images\n",
      "A 45 images\n",
      "Ba 46 images\n",
      "Be 46 images\n",
      "Bi 46 images\n",
      "Bo 46 images\n",
      "Bu 46 images\n",
      "Bya 46 images\n",
      "Byo 46 images\n",
      "Byu 46 images\n",
      "Cha 46 images\n",
      "Che 46 images\n",
      "Chi 46 images\n",
      "Cho 46 images\n",
      "Chu 46 images\n",
      "Da 46 images\n",
      "De 46 images\n",
      "Di 46 images\n",
      "Do 46 images\n",
      "Du 46 images\n",
      "Dyu 46 images\n",
      "E 46 images\n",
      "Fa 46 images\n",
      "Fe 46 images\n",
      "Fi 46 images\n",
      "Fo 45 images\n",
      "Fu 46 images\n",
      "Ga 46 images\n",
      "Ge 46 images\n",
      "Gi 46 images\n",
      "Go 46 images\n",
      "Gu 46 images\n",
      "Gya 46 images\n",
      "Gyo 46 images\n",
      "Gyu 46 images\n",
      "Ha 46 images\n",
      "He 46 images\n",
      "Hi 46 images\n",
      "Ho 46 images\n",
      "Hya 46 images\n",
      "Hyo 46 images\n",
      "Hyu 46 images\n",
      "I 46 images\n",
      "Ja 46 images\n",
      "Je 46 images\n",
      "Ji 46 images\n",
      "Ji_2 46 images\n",
      "Jo 46 images\n",
      "Ju 46 images\n",
      "Ka 46 images\n",
      "Ke 46 images\n",
      "Ki 46 images\n",
      "Ko 46 images\n",
      "Ku 46 images\n",
      "Kya 46 images\n",
      "Kyo 46 images\n",
      "Kyu 46 images\n",
      "Ma 46 images\n",
      "Me 46 images\n",
      "Mi 46 images\n",
      "Mo 46 images\n",
      "Mu 46 images\n",
      "Mya 46 images\n",
      "Myo 46 images\n",
      "Myu 46 images\n",
      "N 46 images\n",
      "Na 46 images\n",
      "Ne 46 images\n",
      "Ni 46 images\n",
      "No 46 images\n",
      "Nu 46 images\n",
      "Nya 46 images\n",
      "Nyo 46 images\n",
      "Nyu 46 images\n",
      "O 46 images\n",
      "Pa 46 images\n",
      "Pe 46 images\n",
      "Pi 46 images\n",
      "Po 46 images\n",
      "Pu 46 images\n",
      "Pya 46 images\n",
      "Pyo 46 images\n",
      "Pyu 46 images\n",
      "Ra 46 images\n",
      "Re 46 images\n",
      "Ri 46 images\n",
      "Ro 46 images\n",
      "Ru 46 images\n",
      "Rya 46 images\n",
      "Ryo 46 images\n",
      "Ryu 46 images\n",
      "Sa 46 images\n",
      "Se 46 images\n",
      "Sha 46 images\n",
      "She 46 images\n",
      "Shi 46 images\n",
      "Sho 46 images\n",
      "Shu 46 images\n",
      "So 46 images\n",
      "Su 46 images\n",
      "Ta 46 images\n",
      "Te 46 images\n",
      "Ti 46 images\n",
      "To 46 images\n",
      "Tsa 46 images\n",
      "Tse 46 images\n",
      "Tso 46 images\n",
      "Tsu 46 images\n",
      "Tu 46 images\n",
      "U 46 images\n",
      "Wa 46 images\n",
      "We 46 images\n",
      "Wi 46 images\n",
      "Wo 46 images\n",
      "Wo_2 46 images\n",
      "Ya 46 images\n",
      "Yo 46 images\n",
      "Yu 46 images\n",
      "Za 46 images\n",
      "Ze 46 images\n",
      "Zo 46 images\n",
      "Zu 46 images\n",
      "Zu_2 46 images\n",
      "Test Data\n",
      "A 23 images\n",
      "Ba 23 images\n",
      "Be 23 images\n",
      "Bi 23 images\n",
      "Bo 23 images\n",
      "Bu 23 images\n",
      "Bya 23 images\n",
      "Byo 23 images\n",
      "Byu 23 images\n",
      "Cha 23 images\n",
      "Chi 23 images\n",
      "Cho 23 images\n",
      "Chu 23 images\n",
      "Da 23 images\n",
      "De 23 images\n",
      "Do 23 images\n",
      "E 23 images\n",
      "Fu 23 images\n",
      "Ga 23 images\n",
      "Ge 23 images\n",
      "Gi 23 images\n",
      "Go 23 images\n",
      "Gu 23 images\n",
      "Gya 23 images\n",
      "Gyo 23 images\n",
      "Gyu 23 images\n",
      "Ha 23 images\n",
      "He 23 images\n",
      "Hi 23 images\n",
      "Ho 23 images\n",
      "Hya 23 images\n",
      "Hyo 23 images\n",
      "Hyu 23 images\n",
      "I 23 images\n",
      "Ja 23 images\n",
      "Ji 23 images\n",
      "Ji_2 23 images\n",
      "Jo 23 images\n",
      "Ju 23 images\n",
      "Ka 23 images\n",
      "Ke 23 images\n",
      "Ki 23 images\n",
      "Ko 23 images\n",
      "Ku 23 images\n",
      "Kya 23 images\n",
      "Kyo 23 images\n",
      "Kyu 23 images\n",
      "Ma 23 images\n",
      "Me 23 images\n",
      "Mi 23 images\n",
      "Mo 23 images\n",
      "Mu 23 images\n",
      "Mya 23 images\n",
      "Myo 23 images\n",
      "Myu 23 images\n",
      "N 23 images\n",
      "Na 23 images\n",
      "Ne 23 images\n",
      "Ni 23 images\n",
      "No 23 images\n",
      "Nu 23 images\n",
      "Nya 23 images\n",
      "Nyo 23 images\n",
      "Nyu 23 images\n",
      "O 23 images\n",
      "Pa 23 images\n",
      "Pe 23 images\n",
      "Pi 23 images\n",
      "Po 23 images\n",
      "Pu 23 images\n",
      "Pya 23 images\n",
      "Pyo 23 images\n",
      "Pyu 23 images\n",
      "Ra 23 images\n",
      "Re 23 images\n",
      "Ri 23 images\n",
      "Ro 23 images\n",
      "Ru 23 images\n",
      "Rya 23 images\n",
      "Ryo 23 images\n",
      "Ryu 23 images\n",
      "Sa 23 images\n",
      "Se 23 images\n",
      "Sha 23 images\n",
      "Shi 23 images\n",
      "Sho 23 images\n",
      "Shu 23 images\n",
      "So 23 images\n",
      "Su 23 images\n",
      "Ta 23 images\n",
      "Te 23 images\n",
      "To 23 images\n",
      "Tsu 23 images\n",
      "U 23 images\n",
      "Wa 23 images\n",
      "Wo 23 images\n",
      "Ya 23 images\n",
      "Yo 23 images\n",
      "Yu 23 images\n",
      "Za 23 images\n",
      "Ze 23 images\n",
      "Zo 23 images\n",
      "Zu 23 images\n",
      "Zu_2 23 images\n",
      "A 23 images\n",
      "Ba 23 images\n",
      "Be 23 images\n",
      "Bi 23 images\n",
      "Bo 23 images\n",
      "Bu 23 images\n",
      "Bya 23 images\n",
      "Byo 23 images\n",
      "Byu 23 images\n",
      "Cha 23 images\n",
      "Che 23 images\n",
      "Chi 23 images\n",
      "Cho 23 images\n",
      "Chu 23 images\n",
      "Da 23 images\n",
      "De 23 images\n",
      "Di 23 images\n",
      "Do 23 images\n",
      "Du 23 images\n",
      "Dyu 23 images\n",
      "E 23 images\n",
      "Fa 23 images\n",
      "Fe 23 images\n",
      "Fi 23 images\n",
      "Fo 23 images\n",
      "Fu 23 images\n",
      "Ga 23 images\n",
      "Ge 23 images\n",
      "Gi 23 images\n",
      "Go 23 images\n",
      "Gu 23 images\n",
      "Gya 23 images\n",
      "Gyo 23 images\n",
      "Gyu 23 images\n",
      "Ha 23 images\n",
      "He 23 images\n",
      "Hi 23 images\n",
      "Ho 23 images\n",
      "Hya 23 images\n",
      "Hyo 23 images\n",
      "Hyu 23 images\n",
      "I 23 images\n",
      "Ja 23 images\n",
      "Je 23 images\n",
      "Ji 23 images\n",
      "Ji_2 23 images\n",
      "Jo 23 images\n",
      "Ju 23 images\n",
      "Ka 23 images\n",
      "Ke 23 images\n",
      "Ki 23 images\n",
      "Ko 23 images\n",
      "Ku 23 images\n",
      "Kya 23 images\n",
      "Kyo 23 images\n",
      "Kyu 23 images\n",
      "Ma 23 images\n",
      "Me 23 images\n",
      "Mi 23 images\n",
      "Mo 23 images\n",
      "Mu 23 images\n",
      "Mya 23 images\n",
      "Myo 23 images\n",
      "Myu 23 images\n",
      "N 23 images\n",
      "Na 23 images\n",
      "Ne 23 images\n",
      "Ni 23 images\n",
      "No 23 images\n",
      "Nu 23 images\n",
      "Nya 23 images\n",
      "Nyo 23 images\n",
      "Nyu 23 images\n",
      "O 23 images\n",
      "Pa 23 images\n",
      "Pe 23 images\n",
      "Pi 23 images\n",
      "Po 23 images\n",
      "Pu 23 images\n",
      "Pya 23 images\n",
      "Pyo 23 images\n",
      "Pyu 23 images\n",
      "Ra 23 images\n",
      "Re 23 images\n",
      "Ri 23 images\n",
      "Ro 23 images\n",
      "Ru 23 images\n",
      "Rya 23 images\n",
      "Ryo 23 images\n",
      "Ryu 23 images\n",
      "Sa 23 images\n",
      "Se 23 images\n",
      "Sha 23 images\n",
      "She 23 images\n",
      "Shi 23 images\n",
      "Sho 23 images\n",
      "Shu 23 images\n",
      "So 23 images\n",
      "Su 23 images\n",
      "Ta 23 images\n",
      "Te 23 images\n",
      "Ti 23 images\n",
      "To 23 images\n",
      "Tsa 23 images\n",
      "Tse 23 images\n",
      "Tso 23 images\n",
      "Tsu 23 images\n",
      "Tu 23 images\n",
      "U 23 images\n",
      "Wa 23 images\n",
      "We 23 images\n",
      "Wi 23 images\n",
      "Wo 23 images\n",
      "Wo_2 23 images\n",
      "Ya 23 images\n",
      "Yo 23 images\n",
      "Yu 23 images\n",
      "Za 23 images\n",
      "Ze 23 images\n",
      "Zo 23 images\n",
      "Zu 23 images\n",
      "Zu_2 23 images\n"
     ]
    }
   ],
   "source": [
    "print('Train Data')\n",
    "for label in letter_labels:\n",
    "    if label[0] >= 104:\n",
    "        print('{} {} images'.format(label[1],\n",
    "                                    len(os.listdir(os.path.join(katakana_train_dir,\n",
    "                                                                \"Katakana_\"+label[2])))))\n",
    "    else :\n",
    "        print('{} {} images'.format(label[1],\n",
    "                                    len(os.listdir(os.path.join(hiragana_train_dir,\n",
    "                                                                \"Hiragana_\"+label[2])))))\n",
    "\n",
    "print('Test Data')\n",
    "for label in letter_labels:\n",
    "    if label[0] >= 104:\n",
    "        print('{} {} images'.format(label[1],\n",
    "                                    len(os.listdir(os.path.join(katakana_test_dir,\n",
    "                                                                \"Katakana_\"+label[2])))))\n",
    "    else :\n",
    "        print('{} {} images'.format(label[1],\n",
    "                                    len(os.listdir(os.path.join(hiragana_test_dir,\n",
    "                                                                \"Hiragana_\"+label[2])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(data, k_dir, h_dir):\n",
    "    for label_num, label in enumerate(letter_labels):\n",
    "        if label[0] >= 104:\n",
    "            for file in os.listdir(os.path.join(k_dir, \"Katakana_\"+label[1])):\n",
    "                data.append(['{}/{}/{}'.format(k_dir,\n",
    "                                                \"Katakana_\"+label[1],\n",
    "                                                file),\n",
    "                                label_num, label[1]])\n",
    "        else :\n",
    "            for file in os.listdir(os.path.join(h_dir, \"Hiragana_\"+label[1])):\n",
    "                data.append(['{}/{}/{}'.format(h_dir,\n",
    "                                                \"Hiragana_\"+label[1],\n",
    "                                                file),\n",
    "                                label_num, label[1]])\n",
    "    return data\n",
    "\n",
    "# def input_data(data, k_dir, h_dir, fold):\n",
    "#     for label_num, label in enumerate(letter_labels):\n",
    "#         if label[0] >= 104:\n",
    "#             for i in fold:\n",
    "#                 for file in os.listdir(os.path.join(k_dir, \"Katakana_\"+label[1])):\n",
    "#                     if file[-5:-4] == i:\n",
    "#                         data.append(['{}/{}/{}'.format(k_dir,\n",
    "#                                                         \"Katakana_\"+label[1],\n",
    "#                                                         file),\n",
    "#                                       label_num, label[1]])\n",
    "#         else :\n",
    "#             for i in fold:\n",
    "#                 for file in os.listdir(os.path.join(h_dir, \"Hiragana_\"+label[1])):\n",
    "#                     if file[-5:-4] == i:\n",
    "#                         data.append(['{}/{}/{}'.format(h_dir,\n",
    "#                                                         \"Hiragana_\"+label[1],\n",
    "#                                                         file),\n",
    "#                                       label_num, label[1]])\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:  (10393, 3)\n",
      "                                           file  label_num english_word\n",
      "0   resize6/train/Hiragana/Hiragana_A/1 a 1.jpg          0            A\n",
      "1   resize6/train/Hiragana/Hiragana_A/1 a 2.jpg          0            A\n",
      "2  resize6/train/Hiragana/Hiragana_A/10 a 1.jpg          0            A\n",
      "3  resize6/train/Hiragana/Hiragana_A/10 a 2.jpg          0            A\n",
      "4  resize6/train/Hiragana/Hiragana_A/11 a 1.jpg          0            A\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "\n",
    "# fold_train_1 = [\"1\",\"2\"]\n",
    "# fold_train_2 = [\"1\",\"3\"]\n",
    "# fold_train_3 = [\"2\",\"3\"]\n",
    "\n",
    "train = input_data(train, katakana_train_dir, hiragana_train_dir)\n",
    "# train = input_data(train, katakana_dir, hiragana_dir, fold_train_1)\n",
    "# train = input_data(train, katakana_dir, hiragana_dir, fold_train_2)\n",
    "# train = input_data(train, katakana_dir, hiragana_dir, fold_train_3)\n",
    "\n",
    "train = pd.DataFrame(train, columns=['file', 'label_num', 'english_word'])\n",
    "\n",
    "print('Training Data: ',train.shape)\n",
    "print(train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 628992 into shape (66,66,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     22\u001b[0m     thinned \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mximgproc\u001b[38;5;241m.\u001b[39mthinning(img_dilation)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#     img_dilation = cv2.dilate(blackAndWhiteImage,\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#                               np.ones((3, 3), np.uint8),\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#                               iterations=1)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#     img_open = cv2.morphologyEx(blackAndWhiteImage, cv2.MORPH_OPEN, kernel=np.ones((3, 3), np.uint8))\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#     img_stack = np.reshape(thinned,(63,63,1))\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     img_stack \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthinned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m66\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m66\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     img_stack = np.reshape(thinned,(69,69,1))\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#     img_stack = np.reshape(img_erotion,(72,72,1))\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#     x_train.append(np.concatenate((np.array(img_resize),\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#                                    np.array(img_stack)), axis=2))\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     x_train\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39marray(img),\n\u001b[0;32m     35\u001b[0m                                    np\u001b[38;5;241m.\u001b[39marray(img_stack)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 628992 into shape (66,66,1)"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    file_name = format(train['file'][i])\n",
    "    img = cv2.imread(file_name)\n",
    "#     img_resize = cv2.resize(img,dsize=(63,63))\n",
    "#     img_resize = cv2.resize(img,dsize=(66,66))\n",
    "#     img_resize = cv2.resize(img,dsize=(69,69))\n",
    "#     img_resize = cv2.resize(img,dsize=(72,72))\n",
    "#     grayImage = cv2.bitwise_not(cv2.cvtColor(img_resize,\n",
    "#                                              cv2.COLOR_BGR2GRAY))\n",
    "    grayImage = cv2.bitwise_not(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "#     (thresh, blackAndWhiteImage) = cv2.threshold(grayImage,\n",
    "#                                                  127, 1,\n",
    "#                                                  cv2.THRESH_BINARY)\n",
    "#     img_erotion = cv2.erode(img_dilation,\n",
    "#                             np.ones((3, 3), np.uint8),\n",
    "#                             iterations=2)\n",
    "    img_dilation = cv2.dilate(grayImage,\n",
    "                              np.ones((3, 3), np.uint8),\n",
    "                              iterations=2)\n",
    "    thinned = cv2.ximgproc.thinning(img_dilation)\n",
    "#     img_dilation = cv2.dilate(blackAndWhiteImage,\n",
    "#                               np.ones((3, 3), np.uint8),\n",
    "#                               iterations=1)\n",
    "#     img_open = cv2.morphologyEx(blackAndWhiteImage, cv2.MORPH_OPEN, kernel=np.ones((3, 3), np.uint8))\n",
    "    \n",
    "#     img_stack = np.reshape(thinned,(63,63,1))\n",
    "    img_stack = np.reshape(thinned,(66,66,1))\n",
    "#     img_stack = np.reshape(thinned,(69,69,1))\n",
    "#     img_stack = np.reshape(img_erotion,(72,72,1))\n",
    "#     x_train.append(np.concatenate((np.array(img_resize),\n",
    "#                                    np.array(img_stack)), axis=2))\n",
    "    x_train.append(np.concatenate((np.array(img),\n",
    "                                   np.array(img_stack)), axis=2))\n",
    "\n",
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Input_image = cv2.imread(train['file'][len(train)-1])\n",
    "\n",
    "plt.imshow(Input_image)\n",
    "plt.title('Input image, Shape: '+str(Input_image.shape))\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(img_resize)\n",
    "# plt.title('Resized image, Shape: '+str(img_resize.shape))\n",
    "plt.imshow(img)\n",
    "plt.title('Resized image, Shape: '+str(img.shape))\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(np.reshape(img_stack,(63,63)))\n",
    "plt.imshow(np.reshape(img_stack,(66,66)))\n",
    "# plt.imshow(np.reshape(img_stack,(69,69)))\n",
    "# plt.imshow(np.reshape(img_stack,(72,72)))\n",
    "plt.title('Processed image, Shape: '+str(img_stack.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = train['label_num']\n",
    "labels = to_categorical(labels, num_classes = len(letter_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train,\n",
    "                                                  labels,\n",
    "                                                  test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_shape = x_train[1].shape\n",
    "print('Input Shape is :', input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "# fold_test_1 = [\"3\"]\n",
    "# fold_test_2 = [\"2\"]\n",
    "# fold_test_3 = [\"1\"]\n",
    "\n",
    "test = input_data(test, katakana_test_dir, hiragana_test_dir)\n",
    "# test = input_data(test, katakana_dir, hiragana_dir, fold_test_1)\n",
    "# test = input_data(test, katakana_dir, hiragana_dir, fold_test_2)\n",
    "# test = input_data(test, katakana_dir, hiragana_dir, fold_test_3)\n",
    "\n",
    "test = pd.DataFrame(test, columns=['file', 'label_num', 'english_word'])\n",
    "print(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_test = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    img = cv2.imread(test['file'][i])\n",
    "#     img_resize = cv2.resize(img,dsize=(63,63))\n",
    "#     img_resize = cv2.resize(img,dsize=(66,66))\n",
    "#     img_resize = cv2.resize(img,dsize=(69,69))\n",
    "#     img_resize = cv2.resize(img,dsize=(72,72))\n",
    "#     grayImage = cv2.bitwise_not(cv2.cvtColor(img_resize,\n",
    "#                                              cv2.COLOR_BGR2GRAY))\n",
    "    grayImage = cv2.bitwise_not(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))\n",
    "#     (thresh, blackAndWhiteImage) = cv2.threshold(grayImage,\n",
    "#                                                  127, 1,\n",
    "#                                                  cv2.THRESH_BINARY)\n",
    "#     img_erotion = cv2.erode(img_dilation,\n",
    "#                             np.ones((3, 3), np.uint8),\n",
    "#                             iterations=2)\n",
    "#     thinned_image = rosenfeld_thinning(blackAndWhiteImage)\n",
    "    img_dilation = cv2.dilate(grayImage,\n",
    "                              np.ones((3, 3), np.uint8),\n",
    "                              iterations=2)\n",
    "    thinned = cv2.ximgproc.thinning(img_dilation)\n",
    "#     img_stack = np.reshape(thinned,(63,63,1))\n",
    "    img_stack = np.reshape(thinned,(66,66,1))\n",
    "#     img_stack = np.reshape(thinned,(69,69,1))\n",
    "#     img_stack = np.reshape(img_erotion,(72,72,1))\n",
    "#     x_test.append(np.concatenate((np.array(img_resize),\n",
    "#                                   np.array(img_stack)), axis=2))\n",
    "    x_test.append(np.concatenate((np.array(img),\n",
    "                                  np.array(img_stack)), axis=2))\n",
    "\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test['label_num']\n",
    "y_test = to_categorical(y_test, num_classes = len(letter_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shape = x_test[1].shape\n",
    "print('Test Shape is :', test_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "randm = np.random.randint(0,len(test))\n",
    "\n",
    "img = cv2.imread(test['file'][randm])\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "# plt.imshow(np.reshape(img_stack,(63,63)))\n",
    "plt.imshow(np.reshape(img_stack,(66,66)))\n",
    "# plt.imshow(np.reshape(img_stack,(69,69)))\n",
    "# plt.imshow(np.reshape(img_stack,(72,72)))\n",
    "plt.title('Processed image, Shape: '+str(img_stack.shape))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fire_incepter(x, fire=16, intercept=64):\n",
    "    x = Conv2D(fire, (5,5), strides=(2,2))(x)\n",
    "    x = LeakyReLU(alpha=0.15)(x)\n",
    "\n",
    "    left = Conv2D(intercept, (3,3), padding='same')(x)\n",
    "    left = LeakyReLU(alpha=0.15)(left)\n",
    "\n",
    "    right = Conv2D(intercept, (5,5), padding='same')(x)\n",
    "    right = LeakyReLU(alpha=0.15)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=3)\n",
    "    x = MaxPooling2D(strides=2, pool_size=3)(x)\n",
    "    return x\n",
    "\n",
    "def fire_squeezer(x, fire=16, intercept=64):\n",
    "    x = Conv2D(fire, (1,1))(x)\n",
    "    x = LeakyReLU(alpha=0.15)(x)\n",
    "\n",
    "    left = Conv2D(intercept, (1,1))(x)\n",
    "    left = LeakyReLU(alpha=0.15)(left)\n",
    "\n",
    "    right = Conv2D(intercept, (3,3), padding='same')(x)\n",
    "    right = LeakyReLU(alpha=0.15)(right)\n",
    "\n",
    "    x = concatenate([left, right], axis=3)\n",
    "    x = MaxPooling2D(strides=2, pool_size=2)(x)\n",
    "    return x\n",
    "\n",
    "image_input=Input(shape=input_shape)\n",
    "\n",
    "x = fire_incepter(image_input, fire=16, intercept=16)\n",
    "\n",
    "x = fire_squeezer(x, fire=64, intercept=64)\n",
    "\n",
    "# x = fire_squeezer(x, fire=64, intercept=64)\n",
    "\n",
    "x = Conv2D(64, (2,2))(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(512)(x)\n",
    "x = LeakyReLU(alpha=0.1)(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "out = Dense(len(letter_labels), activation='softmax')(x)\n",
    "\n",
    "model_new = Model(image_input, out)\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_new.compile(optimizer = Nadam(learning_rate=0.001) ,\n",
    "                  loss = 'categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "history = model_new.fit(x=x_train, y=y_train, validation_data=(x_val, y_val),\n",
    "                        batch_size = 32, epochs=epochs,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss',\n",
    "#                         callbacks=[EarlyStopping(monitor='val_accuracy',\n",
    "                                    patience=3,\n",
    "#                                     patience=0, \n",
    "                                    mode='auto')]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Training for ' +str(epochs)+ ' epochs')\n",
    "plt.legend(['Training accuracy', 'Validation accuracy'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Training for ' +str(epochs)+ ' epochs')\n",
    "plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score = model_new.evaluate(x_val,y_val)\n",
    "print('Accuracy on Validation Set', round(score[1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Pred_labels = np.argmax(model_new.predict(x_test),axis = 1)\n",
    "\n",
    "Pred_labels = pd.DataFrame(Pred_labels,index =None,columns=['label_num'])\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['label_num'] = Pred_labels['label_num']\n",
    "test_df['file'] = test['file']\n",
    "test_df['english_word'] = [letter_labels[i][2] for i in Pred_labels['label_num']]\n",
    "\n",
    "submission = pd.merge(left=sample_submission, right=test_df[['file',\n",
    "                                                             'english_word']],\n",
    "                      on=\"file\", how=\"right\")\n",
    "submission.drop(['english_word_x'], axis = 1, inplace = True)\n",
    "submission.columns = ['file','english_word']\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(submission.head())\n",
    "\n",
    "print('')\n",
    "print('Data test Accuracy : ', round(accuracy_score(np.argmax(y_test, axis=1), Pred_labels),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_feat = Model(inputs=model_new.input,outputs=out)\n",
    "\n",
    "feat_train = model_feat.predict(x_train)\n",
    "print(feat_train.shape)\n",
    "print('')\n",
    "\n",
    "# svm = SVC(kernel='linear')\n",
    "# svm = SVC(kernel='poly', degree=2)\n",
    "# svm = SVC(kernel='poly', degree=3)\n",
    "svm = SVC(kernel='rbf')\n",
    "# svm = SVC(kernel='sigmoid')\n",
    "\n",
    "svm.fit(feat_train,np.argmax(y_train,axis=1))\n",
    "\n",
    "print('fitting done !!!')\n",
    "\n",
    "round(svm.score(feat_train,np.argmax(y_train,axis=1)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feat_val = model_feat.predict(x_val)\n",
    "print(feat_val.shape)\n",
    "\n",
    "round(svm.score(feat_val,np.argmax(y_val,axis=1)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feat_test = model_feat.predict(x_test)\n",
    "print(feat_test.shape)\n",
    "print('')\n",
    "\n",
    "Pred_labels1 = svm.predict(feat_test)\n",
    "\n",
    "Pred_labels = pd.DataFrame(Pred_labels1,index =None,columns=['label_num'])\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['label_num'] = Pred_labels['label_num']\n",
    "test_df['file'] = test['file']\n",
    "test_df['english_word'] = [letter_labels[i][2] for i in Pred_labels['label_num']]\n",
    "\n",
    "submission = pd.merge(left=sample_submission, right=test_df[['file',\n",
    "                                                             'english_word']],\n",
    "                      on=\"file\", how=\"right\")\n",
    "submission.drop(['english_word_x'], axis = 1, inplace = True)\n",
    "submission.columns = ['file','english_word'] \n",
    "submission.to_csv('submission_svm.csv', index=False)\n",
    "\n",
    "print(submission.head())\n",
    "\n",
    "print('')\n",
    "print('Data test Accuracy : ', round(accuracy_score(np.argmax(y_test, axis=1), Pred_labels),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "y_test2 = np.argmax(y_test, axis=1)\n",
    "\n",
    "print('Loss :', round(hamming_loss(y_test2, Pred_labels1),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
